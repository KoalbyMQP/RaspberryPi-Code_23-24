import json
import math
from pathlib import Path
import depthai as dai
import cv2
import numpy as np

from .preview_manager import PreviewManager, SyncedPreviewManager
from ..previews import Previews
from ..utils import loadModule, toTensorResult, frameNorm, toPlanar


class NNetManager:
    """
    Manager class handling all NN-related functionalities. It's capable of creating appropriate nodes and connections,
    decoding neural network output automatically or by using external handler file.
    """

    def __init__(self, inputSize, nnFamily=None, labels=[], confidence=0.5, sync=False, device=None):
        """
        Args:
            inputSize (tuple): Desired NN input size, should match the input size defined in the network itself (width, height)
            nnFamily (str, Optional): type of NeuralNetwork to be processed. Supported: :code:`"YOLO"` and :code:`mobilenet`
            labels (list, Optional): Allows to display class label instead of ID when drawing nn detections.
            confidence (float, Optional): Specify detection nn's confidence threshold
            sync (bool, Optional): Store NN results for preview syncing (to be used with SyncedPreviewManager
        """
        self.inputSize = inputSize
        self._nnFamily = nnFamily
        if nnFamily in ("YOLO", "mobilenet"):
            self._outputFormat = "detection"
        self._labels = labels
        self._confidence = confidence
        self._sync = sync
        self.device = device
        self.calibData = None
        self.cameraIntrinsics = None
        self.distCoeffs = None
    
    
    def getHFov(intrinsics, width):
        fx = intrinsics[0][0]
        fov = 2 * 180 / (math.pi) * math.atan(width * 0.5 / fx)
        return fov

    def getVFov(intrinsics, height):
        fy = intrinsics[1][1]
        fov = 2 * 180 / (math.pi) * math.atan(height * 0.5 / fy)
        return fov

    def getDFov(intrinsics, w, h):
        fx = intrinsics[0][0]
        fy = intrinsics[1][1]
        return np.degrees(2*np.arctan(np.sqrt(w*w+h*h)/(((fx + fy)))))

    def initializeCalibration(self):
        """Initialize camera calibration data from device following official implementation"""
        if self.calibData is not None:  # Already initialized
            return
            
        if self.device is None:
            raise RuntimeError("Device not initialized! Pass device instance during NNetManager initialization.")
            
        self.calibData = self.device.readCalibration()
        
        # Get camera board type
        eeprom = self.calibData.getEepromData()
        if "OAK-1" in eeprom.boardName or "BW1093OAK" in eeprom.boardName:
            # Handle OAK-1 case
            self.cameraIntrinsics = np.array(self.calibData.getCameraIntrinsics(dai.CameraBoardSocket.CAM_A, 1280, 720))
            self.distCoeffs = np.array(self.calibData.getDistortionCoefficients(dai.CameraBoardSocket.CAM_A))
            self.fov = self.calibData.getFov(dai.CameraBoardSocket.CAM_A)
        else:
            # Handle other OAK cameras (OAK-D, etc)
            M_rgb, width, height = self.calibData.getDefaultIntrinsics(dai.CameraBoardSocket.CAM_A)
            self.cameraIntrinsics = np.array(M_rgb)
            
            # Get resized intrinsics for actual frame size (1920x1080 for RGB)
            self.cameraIntrinsics = np.array(self.calibData.getCameraIntrinsics(dai.CameraBoardSocket.CAM_A, 
                                                                            1920, 1080))
            
            self.distCoeffs = np.array(self.calibData.getDistortionCoefficients(dai.CameraBoardSocket.CAM_A))
            self.fov = self.calibData.getFov(dai.CameraBoardSocket.CAM_A)
            
            # Calculate FOVs using official methods
            self.hFov = self.getHFov(self.cameraIntrinsics, 1920)
            self.vFov = self.getVFov(self.cameraIntrinsics, 1080)
            self.dFov = self.getDFov(self.cameraIntrinsics, 1920, 1080)
            
        return self.cameraIntrinsics, self.distCoeffs, self.fov

    def getCameraIntrinsics(self, frame_width=None, frame_height=None):
        """Get camera intrinsics matrix and distortion coefficients"""
        if self.calibData is None:
            self.initializeCalibration()
        
        if frame_width is not None and frame_height is not None:
            # Return resized intrinsics if dimensions are provided
            return (np.array(self.calibData.getCameraIntrinsics(dai.CameraBoardSocket.RGB, 
                                                            frame_width, frame_height)),
                    self.distCoeffs,
                    self.fov)
        
        return self.cameraIntrinsics, self.distCoeffs, self.fov

    #: list: List of available neural network inputs
    sourceChoices = ("color", "left", "right", "rectifiedLeft", "rectifiedRight", "host")
    #: str: Selected neural network input
    source = None
    #: tuple: NN input size (width, height)
    inputSize = None
    #: depthai.OpenVINO.Version: OpenVINO version, available only if parsed from config file (see :func:`readConfig`)
    openvinoVersion = None
    #: depthai.DataInputQueue: DepthAI input queue object that allows to send images from host to device (used only with :code:`host` source)
    inputQueue = None
    #: depthai.DataOutputQueue: DepthAI output queue object that allows to receive NN results from the device.
    outputQueue = None
    #: dict: nn data buffer, disabled by default. Stores parsed nn data with packet sequence number as dict key
    buffer = {}


    _bboxColors = np.random.random(size=(256, 3)) * 256  # Random Colors for bounding boxes
    _countLabel = None
    _textBgColor = (0, 0, 0)
    _textColor = (255, 255, 255)
    _lineType = cv2.LINE_AA
    _textType = cv2.FONT_HERSHEY_SIMPLEX
    _outputFormat = "raw"
    _metadata = None
    _fullFov = True
    _config = None
    _nnFamily = None
    _handler = None

    def readConfig(self, path):
        """
        Parses the model config file and adjusts NNetManager values accordingly. It's advised to create a config file
        for every new network, as it allows to use dedicated NN nodes (for `MobilenetSSD <https://github.com/luxonis/depthai/blob/main/resources/nn/mobilenet-ssd/mobilenet-ssd.json>`__ and `YOLO <https://github.com/luxonis/depthai/blob/main/resources/nn/tiny-yolo-v3/tiny-yolo-v3.json>`__)
        or use `custom handler <https://github.com/luxonis/depthai/blob/main/resources/nn/openpose2/openpose2.json>`__ to process and display custom network results

        Args:
            path (pathlib.Path): Path to model config file (.json)

        Raises:
            ValueError: If path to config file does not exist
            RuntimeError: If custom handler does not contain :code:`draw` or :code:`show` methods
        """
        configPath = Path(path)
        if not configPath.exists():
            raise ValueError("Path {} does not exist!".format(path))

        with configPath.open() as f:
            self._config = json.load(f)
            if "openvino_version" in self._config:
                self.openvinoVersion =getattr(dai.OpenVINO.Version, 'VERSION_' + self._config.get("openvino_version"))
            nnConfig = self._config.get("nn_config", {})
            self._labels = self._config.get("mappings", {}).get("labels", None)
            self._nnFamily = nnConfig.get("NN_family", None)
            self._outputFormat = nnConfig.get("output_format", "raw")
            self._metadata = nnConfig.get("NN_specific_metadata", {})
            if "input_size" in nnConfig:
                self.inputSize = tuple(map(int, nnConfig.get("input_size").split('x')))

            self._confidence = self._metadata.get("confidence_threshold", nnConfig.get("confidence_threshold", None))
            if 'handler' in self._config:
                self._handler = loadModule(configPath.parent / self._config["handler"])

                if not callable(getattr(self._handler, "draw", None)) or not callable(getattr(self._handler, "decode", None)):
                    raise RuntimeError("Custom model handler does not contain 'draw' or 'decode' methods!")


    def _normFrame(self, frame):
        if not self._fullFov:
            scaleF = frame.shape[0] / self.inputSize[1]
            return np.zeros((int(self.inputSize[1] * scaleF), int(self.inputSize[0] * scaleF)))
        else:
            return frame

    def _cropOffsetX(self, frame):
        if not self._fullFov:
            croppedW = (frame.shape[0] / self.inputSize[1]) * self.inputSize[0]
            return int((frame.shape[1] - croppedW) // 2)
        else:
            return 0

    def createNN(self, pipeline, nodes, blobPath, source="color", useDepth=False, minDepth=100, maxDepth=10000, sbbScaleFactor=0.3, fullFov=True, useImageManip=True, device=None):
        """
        Creates nodes and connections in provided pipeline that will allow to run NN model and consume it's results.

        Args:
            pipeline (depthai.Pipeline): Pipeline instance
            nodes (types.SimpleNamespace): Object cointaining all of the nodes added to the pipeline. Available in :attr:`depthai_sdk.managers.PipelineManager.nodes`
            blobPath (pathlib.Path): Path to MyriadX blob. Might be useful to use together with
                :func:`depthai_sdk.managers.BlobManager.getBlob()` for dynamic blob compilation
            source (str, Optional): Neural network input source, one of :attr:`sourceChoices`
            useDepth (bool, Optional): If set to True, produced detections will have spatial coordinates included
            minDepth (int, Optional): Minimum depth distance in centimeters
            maxDepth (int, Optional): Maximum depth distance in centimeters
            sbbScaleFactor (float, Optional): Scale of the bounding box that will be used to calculate spatial coordinates for
                detection. If set to 0.3, it will scale down center-wise the bounding box to 0.3 of it's original size
                and use it to calculate spatial location of the object
            fullFov (bool, Optional): If set to False, manager will include crop offset when scaling the detections.
                Usually should be set to True (if you don't perform aspect ratio crop or when `keepAspectRatio` flag
                on camera/manip node is set to False
            useImageManip (bool, Optional): If set to False, manager will not create an image manip node for input image
                scaling - which may result in an input image being not adjusted for the NeuralNetwork node. Can be useful
                when we want to limit the amount of nodes running simultaneously on device

        Returns:
            depthai.node.NeuralNetwork: Configured NN node that was added to the pipeline

        Raises:
            RuntimeError: If source is not a valid choice or when input size has not been set.
        """
        if source not in self.sourceChoices:
            raise RuntimeError(f"Source {source} is invalid, available {self.sourceChoices}")
        if self.inputSize is None:
            raise RuntimeError("Unable to determine the nn input size. Please use --cnnInputSize flag to specify it in WxH format: -nnSize <width>x<height>")
        self.device = device
        self.source = source
        self._fullFov = fullFov
        if self._nnFamily == "mobilenet":
            nodes.nn = pipeline.createMobileNetSpatialDetectionNetwork() if useDepth else pipeline.createMobileNetDetectionNetwork()
            nodes.nn.setConfidenceThreshold(self._confidence)
        elif self._nnFamily == "YOLO":
            nodes.nn = pipeline.createYoloSpatialDetectionNetwork() if useDepth else pipeline.createYoloDetectionNetwork()
            nodes.nn.setConfidenceThreshold(self._confidence)
            nodes.nn.setNumClasses(self._metadata["classes"])
            nodes.nn.setCoordinateSize(self._metadata["coordinates"])
            nodes.nn.setAnchors(self._metadata["anchors"])
            nodes.nn.setAnchorMasks(self._metadata["anchor_masks"])
            nodes.nn.setIouThreshold(self._metadata["iou_threshold"])
        else:
            # TODO use createSpatialLocationCalculator
            nodes.nn = pipeline.createNeuralNetwork()

        nodes.nn.setBlobPath(str(blobPath))
        nodes.nn.setNumInferenceThreads(2)
        nodes.nn.input.setBlocking(False)
        nodes.nn.input.setQueueSize(2)

        nodes.xoutNn = pipeline.createXLinkOut()
        nodes.xoutNn.setStreamName("nnOut")
        nodes.nn.out.link(nodes.xoutNn.input)

        if self.source == "host":
            nodes.xinNn = pipeline.createXLinkIn()
            nodes.xinNn.setMaxDataSize(self.inputSize[0] * self.inputSize[1] * 3)
            nodes.xinNn.setStreamName("nnIn")
            nodes.xinNn.out.link(nodes.nn.input)
        else:
            if useImageManip:
                nodes.manipNn = pipeline.createImageManip()
                nodes.manipNn.initialConfig.setResize(*self.inputSize)
                # The NN model expects BGR input. By default ImageManip output type would be same as input (gray in this case)
                nodes.manipNn.initialConfig.setFrameType(dai.RawImgFrame.Type.BGR888p)
                # NN inputs
                nodes.manipNn.out.link(nodes.nn.input)
                nodes.manipNn.setKeepAspectRatio(not self._fullFov)
                nodes.manipNn.setMaxOutputFrameSize(self.inputSize[0] * self.inputSize[1] * 3)

                link_input = nodes.manipNn.inputImage
            else:
                link_input = nodes.nn.input

            if self.source == "color":
                nodes.camRgb.preview.link(link_input)
            if self.source == "left":
                nodes.monoLeft.out.link(link_input)
            elif self.source == "right":
                nodes.monoRight.out.link(link_input)
            elif self.source == "rectifiedLeft":
                nodes.stereo.rectifiedLeft.link(link_input)
            elif self.source == "rectifiedRight":
                nodes.stereo.rectifiedRight.link(link_input)

        if self._nnFamily in ("YOLO", "mobilenet") and useDepth:
            nodes.stereo.depth.link(nodes.nn.inputDepth)
            nodes.nn.setDepthLowerThreshold(minDepth)
            nodes.nn.setDepthUpperThreshold(maxDepth)
            nodes.nn.setBoundingBoxScaleFactor(sbbScaleFactor)

        return nodes.nn

    def getLabelText(self, label):
        """
        Retrieves text assigned to specific label

        Args:
            label (int): Integer representing detection label, usually returned from NN node

        Returns:
            str: Label text assigned to specific label id or label id

        Raises:
            RuntimeError: If source is not a valid choice or when input size has not been set.
        """
        if self._labels is None:
            return str(label)
        elif int(label) < len(self._labels):
            return self._labels[int(label)]
        else:
            print(f"Label of ouf bounds (label index: {label}, available labels: {len(self._labels)}")
            return str(label)

    def parse(self, blocking=False):
        if self.outputQueue is None:
            return None, None

        if blocking:
            inNn = self.outputQueue.get()
        else:
            inNn = self.outputQueue.tryGet()
        if inNn is not None:
            data = self.decode(inNn)
            if self._sync:
                self.buffer[inNn.getSequenceNum()] = data
            return data, inNn
        else:
            return None, None

    def decode(self, inNn):
        """
        Decodes NN output. Performs generic handling for supported detection networks or calls custom handler methods

        Args:
            inNn (depthai.NNData): Integer representing detection label, usually returned from NN node

        Returns:
            Decoded NN data

        Raises:
            RuntimeError: if outputFormat specified in model config file is not recognized
        """
        if self._outputFormat == "detection":
            return inNn.detections
        elif self._outputFormat == "raw":
            if self._handler is not None:
                return self._handler.decode(self, inNn)
            else:
                try:
                    data = toTensorResult(inNn)
                    print("Received NN packet: ", ", ".join([f"{key}: {value.shape}" for key, value in data.items()]))
                except Exception as ex:
                    print("Received NN packet: <Preview unabailable: {}>".format(ex))
        else:
            raise RuntimeError("Unknown output format: {}".format(self._outputFormat))
        
    def calculatePhysicalDimensions(self, detection, frame):
        """Calculate physical dimensions using official focal length approach"""
        # Get position in meters
        xMeters = detection.spatialCoordinates.x / 1000  # Convert mm to meters
        yMeters = detection.spatialCoordinates.y / 1000
        zMeters = detection.spatialCoordinates.z / 1000

        # Get frame dimensions
        frame_width = 1920
        frame_height = 1080
        
        # Calculate focal lengths from intrinsics
        fx = 1504.128173828125
        fy = 1504.2720947265625
        
        # Calculate dimensions in pixels
        bbox_width_pixels = (detection.xmax - detection.xmin) * frame_width
        bbox_height_pixels = (detection.ymax - detection.ymin) * frame_height
        
        # Calculate physical dimensions using similar triangles principle
        physical_width = (bbox_width_pixels * zMeters) / fx
        physical_height = (bbox_height_pixels * zMeters) / fy
        
        return {
            'position': {'x': xMeters, 'y': yMeters, 'z': zMeters},
            'dimensions': {'width': physical_width, 'height': physical_height},
            'area': physical_width * physical_height
        }
        
    # function for future use with chess
    # def calculatePhysicalDimensions(self, detection, frame):
    #     """
    #     Calculate physical dimensions using camera intrinsics and depth.
    #     Handles chess-specific calculations only when a chessboard is detected.
    #     """
    #     # Initialize calibration if needed
    #     if self.cameraIntrinsics is None:
    #         self.initializeCalibration()
        
    #     # Get position in meters
    #     xMeters = detection.spatialCoordinates.x / 1000
    #     yMeters = detection.spatialCoordinates.y / 1000  
    #     zMeters = detection.spatialCoordinates.z / 1000

    #     # Get frame dimensions
    #     frame_width = 1920
    #     frame_height = 1080

    #     # Get focal lengths from intrinsics
    #     fx = self.cameraIntrinsics[0][0]
    #     fy = self.cameraIntrinsics[1][1]

    #     # Calculate dimensions in pixels
    #     bbox_width_pixels = (detection.xmax - detection.xmin) * frame_width
    #     bbox_height_pixels = (detection.ymax - detection.ymin) * frame_height

    #     # Calculate physical dimensions
    #     physical_width = (bbox_width_pixels * zMeters) / fx
    #     physical_height = (bbox_height_pixels * zMeters) / fy

    #     # Basic dimensions dict for non-chess objects
    #     dimensions = {
    #         'position': {
    #             'x': xMeters,
    #             'y': yMeters,
    #             'z': zMeters
    #         },
    #         'dimensions': {
    #             'width': physical_width,
    #             'height': physical_height
    #         },
    #         'area': physical_width * physical_height,
    #         'graspable': physical_width < 0.15 and physical_height < 0.15,  # Max gripper size
    #         'type': 'general_object'
    #     }

    #     # Get label text
    #     label = self.getLabelText(detection.label)

    #     # If this is a chessboard detection, store its coordinates for future reference
    #     if label == "chessboard":
    #         self._last_chessboard = {
    #             'bbox': [detection.xmin, detection.ymin, detection.xmax, detection.ymax],
    #             'physical_size': physical_width,  # Standard chess board is square
    #             'center': {
    #                 'x': (detection.xmin + detection.xmax) / 2,
    #                 'y': (detection.ymin + detection.ymax) / 2
    #             },
    #             'timestamp': time.time()
    #         }
    #         dimensions['type'] = 'chessboard'
    #         dimensions['square_size'] = physical_width / 8  # Standard 8x8 board
    #         return dimensions

    #     # If this is a chess piece AND we have recently seen a chessboard
    #     if (label in ["pawn", "knight", "bishop", "rook", "queen", "king"] and 
    #         hasattr(self, '_last_chessboard') and 
    #         time.time() - self._last_chessboard['timestamp'] < 1.0):  # Within last second
            
    #         # Calculate piece position relative to board
    #         piece_center_x = (detection.xmin + detection.xmax) / 2
    #         piece_center_y = (Detection.ymin + detection.ymax) / 2
            
    #         # Convert to board coordinates
    #         board_bbox = self._last_chessboard['bbox']
    #         board_width = board_bbox[2] - board_bbox[0]
    #         board_height = board_bbox[3] - board_bbox[1]
            
    #         # Normalize position to board coordinates
    #         rel_x = (piece_center_x - board_bbox[0]) / board_width
    #         rel_y = (piece_center_y - board_bbox[1]) / board_height
            
    #         # Convert to chess notation (A1-H8)
    #         file_idx = int(rel_x * 8)
    #         rank_idx = 7 - int(rel_y * 8)  # Invert Y since chess notation starts from bottom
    #         chess_position = f"{chr(65 + file_idx)}{rank_idx + 1}"
            
    #         # Add chess-specific info to dimensions
    #         dimensions.update({
    #             'type': 'chess_piece',
    #             'chess_position': chess_position,
    #             'piece_type': label,
    #             'board_relative_pos': {
    #                 'x': rel_x,
    #                 'y': rel_y
    #             }
    #         })
        
    #     return dimensions

    # backup functions for future use 
    # def detectChessboard(self, frame):
    #     """
    #     Additional helper method to verify chessboard detection using classical CV in case yolo fails
    #     Can be used to validate YOLO's chessboard detection
    #     """
    #     gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        
    #     # Try to find the internal corners of the chessboard
    #     criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 30, 0.001)
    #     ret, corners = cv2.findChessboardCorners(gray, (7,7), None)  # 7x7 internal corners
        
    #     if ret:
    #         corners2 = cv2.cornerSubPix(gray, corners, (11,11), (-1,-1), criteria)
    #         return True, corners2
    #     return False, None
    # def validateChessMove(self, prev_pos, new_pos):
    #     """Validate if detected chess piece movement is legal"""
    #     if not hasattr(self, '_last_chess_positions'):
    #         self._last_chess_positions = {}
        
    #     piece_type = new_pos['piece_type']
    #     old_square = prev_pos['chess_position']
    #     new_square = new_pos['chess_position']
        
    #     # Basic movement validation
    #     file_diff = abs(ord(new_square[0]) - ord(old_square[0]))
    #     rank_diff = abs(int(new_square[1]) - int(old_square[1]))
        
    #     # Simple rule checking
    #     valid = False
    #     if piece_type == 'pawn':
    #         valid = file_diff == 0 and rank_diff <= 2
    #     elif piece_type == 'knight':
    #         valid = (file_diff == 2 and rank_diff == 1) or (file_diff == 1 and rank_diff == 2)
    #     elif piece_type == 'bishop':
    #         valid = file_diff == rank_diff
    #     elif piece_type == 'rook':
    #         valid = file_diff == 0 or rank_diff == 0
    #     elif piece_type == 'queen':
    #         valid = file_diff == rank_diff or file_diff == 0 or rank_diff == 0
    #     elif piece_type == 'king':
    #         valid = file_diff <= 1 and rank_diff <= 1
            
    #     return valid
    
    def _drawCount(self, source, decodedData):
        def drawCnt(frame, cnt):
            cv2.putText(frame, f"{self._countLabel}: {cnt}", (5, 46), self._textType, 0.5, self._textBgColor, 4, self._lineType)
            cv2.putText(frame, f"{self._countLabel}: {cnt}", (5, 46), self._textType, 0.5, self._textColor, 1, self._lineType)

        # Count the number of detected objects
        cntList = list(filter(lambda x: self.getLabelText(x.label) == self._countLabel, decodedData))
        if isinstance(source, PreviewManager):
            for frame in source.frames.values():
                drawCnt(frame, len(cntList))
        else:
            drawCnt(source, len(cntList))
        
    def draw(self, source, decodedData):
        """
        Draws NN results onto the frames. It's responsible to correctly map the results onto each frame requested,
        including applying crop offset or preparing a correct normalization frame, then draws them with all information
        provided (confidence, label, spatial location, label count).

        Also, it's able to call custom nn handler method :code:`draw` to hand over drawing the results

        Args:
            source (depthai_sdk.managers.PreviewManager | numpy.ndarray): Draw target.
                If supplied with a regular frame, it will draw the count on that frame

                If supplied with :class:`depthai_sdk.managers.PreviewManager` instance, it will print the count label
                on all of the frames that it stores

            decodedData: Detections from neural network node, usually returned from :func:`decode` method
        """
        if self._outputFormat == "detection":
            def drawDetection(frame, detection):
                bbox = frameNorm(self._normFrame(frame), [detection.xmin, detection.ymin, detection.xmax, detection.ymax])
                if self.source == Previews.color.name and not self._fullFov:
                    bbox[::2] += self._cropOffsetX(frame)
                
                # Draw bounding box
                cv2.rectangle(frame, (bbox[0], bbox[1]), (bbox[2], bbox[3]), self._bboxColors[detection.label], 2)
                cv2.rectangle(frame, (bbox[0], (bbox[1] - 28)), ((bbox[0] + 110), bbox[1]), self._bboxColors[detection.label], cv2.FILLED)
                cv2.putText(frame, self.getLabelText(detection.label), (bbox[0] + 5, bbox[1] - 10),
                            self._textType, 0.5, (0, 0, 0), 1, self._lineType)
                cv2.putText(frame, f"{int(detection.confidence * 100)}%", (bbox[0] + 62, bbox[1] - 10),
                            self._textType, 0.5, (0, 0, 0), 1, self._lineType)

                if hasattr(detection, 'spatialCoordinates'):
                    # Calculate physical dimensions
                    frame_width = 1920
                    frame_height = 1080
                    measurements = self.calculatePhysicalDimensions(detection, frame)
                    
                    # Draw measurements
                    texts = [
                        f"X: {measurements['position']['x']:.3f}m",
                        f"Y: {measurements['position']['y']:.3f}m",
                        f"Z: {measurements['position']['z']:.3f}m",
                        f"Width: {measurements['dimensions']['width']:.3f}m",
                        f"Height: {measurements['dimensions']['height']:.3f}m",
                        f"Area: {measurements['area']:.3f}m²"
                    ]

                    for i, text in enumerate(texts):
                        cv2.putText(frame, text, (bbox[0] + 10, bbox[1] + 60 + i*15),
                                self._textType, 0.5, self._textBgColor, 4, self._lineType)
                        cv2.putText(frame, text, (bbox[0] + 10, bbox[1] + 60 + i*15),
                                self._textType, 0.5, self._textColor, 1, self._lineType)

            if isinstance(source, SyncedPreviewManager):
                if len(self.buffer) > 0 and source.nnSyncSeq is not None:
                    data = self.buffer.get(source.nnSyncSeq, self.buffer[max(self.buffer.keys())])
                    for old_key in list(filter(lambda key: key < source.nnSyncSeq, self.buffer.keys())):
                        del self.buffer[old_key]
                    for detection in data:
                        for name, frame in source.frames.items():
                            drawDetection(frame, detection)
            else:
                for detection in decodedData:
                    if isinstance(source, PreviewManager):
                        for name, frame in source.frames.items():
                            drawDetection(frame, detection)
                    else:
                        drawDetection(source, detection)

            if self._countLabel is not None:
                self._drawCount(source, decodedData)

        elif self._outputFormat == "raw" and self._handler is not None:
            if isinstance(source, PreviewManager):
                frames = list(source.frames.items())
            else:
                frames = [("host", source)]
            self._handler.draw(self, decodedData, frames)

    def createQueues(self, device):
        """
        Creates output queue for NeuralNetwork node and, if using :code:`host` as a :attr:`source`, it will also create
        input queue.

        Args:
            device (depthai.Device): Running device instance
        """
        if self.source == "host":
            self.inputQueue = device.getInputQueue("nnIn", maxSize=1, blocking=False)
        self.outputQueue = device.getOutputQueue("nnOut", maxSize=1, blocking=False)

    def closeQueues(self):
        """
        Closes output queues created by :func:`createQueues`
        """
        if self.source == "host" and self.inputQueue is not None:
            self.inputQueue.close()
        if self.outputQueue is not None:
            self.outputQueue.close()

    def sendInputFrame(self, frame, seqNum=None):
        """
        Sends a frame into :attr:`inputQueue` object. Handles scaling down the frame, creating a proper :obj:`depthai.ImgFrame`
        and sending it to the queue. Be sure to use :code:`host` as a :attr:`source` and call :func:`createQueues` prior
        input queue.

        Args:
            frame (numpy.ndarray): Frame to be sent to the device
            seqNum (int, Optional): Sequence number set on ImgFrame. Useful in synchronization scenarios

        Returns:
            numpy.ndarray: scaled frame that was sent to the NN (same width/height as NN input)

        Raises:
            RuntimeError: if :attr:`inputQueue` is :code:`None` (unable to send the image)
        """
        if self.inputQueue is None:
            raise RuntimeError("Unable to send image, no input queue is present! Call `createQueues(device)` first!")

        scaledFrame = cv2.resize(frame, self.inputSize)
        frameNn = dai.ImgFrame()
        if seqNum is not None:
            frameNn.setSequenceNum(seqNum)
        frameNn.setType(dai.ImgFrame.Type.BGR888p)
        frameNn.setWidth(self.inputSize[0])
        frameNn.setHeight(self.inputSize[1])
        frameNn.setData(toPlanar(scaledFrame))
        self.inputQueue.send(frameNn)

        return scaledFrame

    def countLabel(self, label):
        """
        Enables object count for specific label. Label count will be printed once :func:`draw` method is called

        Args:
            label (str | int): Label to be counted. If model is using mappings in model config file, supply here a :obj:`str` label
                to be tracked. If no mapping is present, specify the label as :obj:`int` (NN-default)
        """

        self._countLabel = label

    

    